{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N0: 102 imágenes (4.28%)\n",
      "N1: 51 imágenes (2.14%)\n",
      "N10: 73 imágenes (3.06%)\n",
      "N11: 74 imágenes (3.11%)\n",
      "N12: 89 imágenes (3.74%)\n",
      "N13: 72 imágenes (3.02%)\n",
      "N14: 76 imágenes (3.19%)\n",
      "N15: 70 imágenes (2.94%)\n",
      "N16: 71 imágenes (2.98%)\n",
      "N17: 64 imágenes (2.69%)\n",
      "N18: 80 imágenes (3.36%)\n",
      "N19: 57 imágenes (2.39%)\n",
      "N2: 66 imágenes (2.77%)\n",
      "N20: 59 imágenes (2.48%)\n",
      "N21: 99 imágenes (4.16%)\n",
      "N22: 84 imágenes (3.53%)\n",
      "N23: 81 imágenes (3.40%)\n",
      "N24: 95 imágenes (3.99%)\n",
      "N25: 153 imágenes (6.42%)\n",
      "N26: 57 imágenes (2.39%)\n",
      "N27: 49 imágenes (2.06%)\n",
      "N28: 68 imágenes (2.85%)\n",
      "N29: 70 imágenes (2.94%)\n",
      "N3: 99 imágenes (4.16%)\n",
      "N30: 78 imágenes (3.27%)\n",
      "N4: 63 imágenes (2.64%)\n",
      "N5: 50 imágenes (2.10%)\n",
      "N6: 92 imágenes (3.86%)\n",
      "N7: 92 imágenes (3.86%)\n",
      "N8: 69 imágenes (2.90%)\n",
      "N9: 79 imágenes (3.32%)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "root_dir = \"Dataset\"\n",
    "count = []\n",
    "label = []\n",
    "\n",
    "for class_folder in os.listdir(root_dir):\n",
    "    class_folder_path = os.path.join(root_dir, class_folder)\n",
    "    counter = 0\n",
    "    for image in os.listdir(class_folder_path):\n",
    "        counter += 1\n",
    "    count.append(counter)\n",
    "    label.append(class_folder)\n",
    "\n",
    "total_images = sum(count)\n",
    "percentages = [(folder, coun, coun / total_images * 100) for folder, coun in zip(label, count)]\n",
    "\n",
    "for folder, count, percentage in percentages:\n",
    "    print(f\"{folder}: {count} imágenes ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m image \u001b[38;5;241m=\u001b[39m images[i \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(images)]\n\u001b[0;32m     32\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(class_folder_path, image))\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m img\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(class_folder_augmented_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maug_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\jonac\\OneDrive\\Documentos\\Proyecto Lenguajes\\venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\jonac\\OneDrive\\Documentos\\Proyecto Lenguajes\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jonac\\OneDrive\\Documentos\\Proyecto Lenguajes\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\jonac\\OneDrive\\Documentos\\Proyecto Lenguajes\\venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:1372\u001b[0m, in \u001b[0;36mRandomRotation.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m   1369\u001b[0m         fill \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mfloat\u001b[39m(f) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m fill]\n\u001b[0;32m   1370\u001b[0m angle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdegrees)\n\u001b[1;32m-> 1372\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mangle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jonac\\OneDrive\\Documentos\\Proyecto Lenguajes\\venv\\lib\\site-packages\\torchvision\\transforms\\functional.py:1121\u001b[0m, in \u001b[0;36mrotate\u001b[1;34m(img, angle, interpolation, expand, center, fill)\u001b[0m\n\u001b[0;32m   1119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m   1120\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mangle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mangle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcenter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1123\u001b[0m center_f \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m]\n\u001b[0;32m   1124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m center \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jonac\\OneDrive\\Documentos\\Proyecto Lenguajes\\venv\\lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:314\u001b[0m, in \u001b[0;36mrotate\u001b[1;34m(img, angle, interpolation, expand, center, fill)\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be PIL Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(img)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    313\u001b[0m opts \u001b[38;5;241m=\u001b[39m _parse_fill(fill, img)\n\u001b[1;32m--> 314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mrotate(angle, interpolation, expand, center, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopts)\n",
      "File \u001b[1;32mc:\\Users\\jonac\\OneDrive\\Documentos\\Proyecto Lenguajes\\venv\\lib\\site-packages\\PIL\\Image.py:2350\u001b[0m, in \u001b[0;36mImage.rotate\u001b[1;34m(self, angle, resample, expand, center, translate, fillcolor)\u001b[0m\n\u001b[0;32m   2347\u001b[0m     matrix[\u001b[38;5;241m2\u001b[39m], matrix[\u001b[38;5;241m5\u001b[39m] \u001b[38;5;241m=\u001b[39m transform(\u001b[38;5;241m-\u001b[39m(nw \u001b[38;5;241m-\u001b[39m w) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.0\u001b[39m, \u001b[38;5;241m-\u001b[39m(nh \u001b[38;5;241m-\u001b[39m h) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.0\u001b[39m, matrix)\n\u001b[0;32m   2348\u001b[0m     w, h \u001b[38;5;241m=\u001b[39m nw, nh\n\u001b[1;32m-> 2350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2351\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAFFINE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfillcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfillcolor\u001b[49m\n\u001b[0;32m   2352\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jonac\\OneDrive\\Documentos\\Proyecto Lenguajes\\venv\\lib\\site-packages\\PIL\\Image.py:2722\u001b[0m, in \u001b[0;36mImage.transform\u001b[1;34m(self, size, method, data, resample, fill, fillcolor)\u001b[0m\n\u001b[0;32m   2718\u001b[0m         im\u001b[38;5;241m.\u001b[39m__transformer(\n\u001b[0;32m   2719\u001b[0m             box, \u001b[38;5;28mself\u001b[39m, Transform\u001b[38;5;241m.\u001b[39mQUAD, quad, resample, fillcolor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2720\u001b[0m         )\n\u001b[0;32m   2721\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2722\u001b[0m     \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__transformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2723\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfillcolor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m   2724\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m im\n",
      "File \u001b[1;32mc:\\Users\\jonac\\OneDrive\\Documentos\\Proyecto Lenguajes\\venv\\lib\\site-packages\\PIL\\Image.py:2805\u001b[0m, in \u001b[0;36mImage.__transformer\u001b[1;34m(self, box, image, method, data, resample, fill)\u001b[0m\n\u001b[0;32m   2802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m image\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   2803\u001b[0m     resample \u001b[38;5;241m=\u001b[39m Resampling\u001b[38;5;241m.\u001b[39mNEAREST\n\u001b[1;32m-> 2805\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "import shutil\n",
    "from PIL import Image\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(10, fill=(255,)),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.RandomApply([transforms.GaussianBlur(3)], p=0.3),\n",
    "    transforms.RandomCrop(224, padding=4),\n",
    "])\n",
    "\n",
    "folder = \"Dataset\"\n",
    "folder_augmented = \"Dataset_Augmented_max\"\n",
    "\n",
    "for class_folder in os.listdir(folder):\n",
    "    class_folder_path = os.path.join(folder, class_folder)\n",
    "    class_folder_augmented_path = os.path.join(folder_augmented, class_folder)\n",
    "    os.makedirs(class_folder_augmented_path, exist_ok=True)\n",
    "\n",
    "    images = os.listdir(class_folder_path)\n",
    "\n",
    "    for image in images:\n",
    "        src_path = os.path.join(class_folder_path, image)\n",
    "        dst_path = os.path.join(class_folder_augmented_path, image)\n",
    "        shutil.copy(src_path, dst_path)\n",
    "\n",
    "    num_to_add = 153 - len(images)\n",
    "    for i in range(num_to_add):\n",
    "        image = images[i % len(images)]\n",
    "        img = Image.open(os.path.join(class_folder_path, image)).convert(\"RGB\")\n",
    "        img = transform(img)\n",
    "        img.save(os.path.join(class_folder_augmented_path, f\"aug_{i}.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N0: 153 imágenes (3.23%)\n",
      "N1: 153 imágenes (3.23%)\n",
      "N10: 153 imágenes (3.23%)\n",
      "N11: 153 imágenes (3.23%)\n",
      "N12: 153 imágenes (3.23%)\n",
      "N13: 153 imágenes (3.23%)\n",
      "N14: 153 imágenes (3.23%)\n",
      "N15: 153 imágenes (3.23%)\n",
      "N16: 153 imágenes (3.23%)\n",
      "N17: 153 imágenes (3.23%)\n",
      "N18: 153 imágenes (3.23%)\n",
      "N19: 153 imágenes (3.23%)\n",
      "N2: 153 imágenes (3.23%)\n",
      "N20: 153 imágenes (3.23%)\n",
      "N21: 153 imágenes (3.23%)\n",
      "N22: 153 imágenes (3.23%)\n",
      "N23: 153 imágenes (3.23%)\n",
      "N24: 153 imágenes (3.23%)\n",
      "N25: 153 imágenes (3.23%)\n",
      "N26: 153 imágenes (3.23%)\n",
      "N27: 153 imágenes (3.23%)\n",
      "N28: 153 imágenes (3.23%)\n",
      "N29: 153 imágenes (3.23%)\n",
      "N3: 153 imágenes (3.23%)\n",
      "N30: 153 imágenes (3.23%)\n",
      "N4: 153 imágenes (3.23%)\n",
      "N5: 153 imágenes (3.23%)\n",
      "N6: 153 imágenes (3.23%)\n",
      "N7: 153 imágenes (3.23%)\n",
      "N8: 153 imágenes (3.23%)\n",
      "N9: 153 imágenes (3.23%)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "root_dir = \"Dataset_Augmented_max\"\n",
    "count = []\n",
    "label = []\n",
    "\n",
    "for class_folder in os.listdir(root_dir):\n",
    "    class_folder_path = os.path.join(root_dir, class_folder)\n",
    "    counter = 0\n",
    "    for image in os.listdir(class_folder_path):\n",
    "        counter += 1\n",
    "    count.append(counter)\n",
    "    label.append(class_folder)\n",
    "\n",
    "total_images = sum(count)\n",
    "percentages = [(folder, coun, coun / total_images * 100) for folder, coun in zip(label, count)]\n",
    "\n",
    "for folder, count, percentage in percentages:\n",
    "    print(f\"{folder}: {count} imágenes ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entrenamiento: 3782 imágenes\n",
      "Total prueba: 961 imágenes\n",
      "[('Dataset_Augmented_max\\\\N0\\\\n0_46.jpeg', 0), ('Dataset_Augmented_max\\\\N0\\\\n0_22.jpeg', 0), ('Dataset_Augmented_max\\\\N0\\\\aug_29.jpg', 0), ('Dataset_Augmented_max\\\\N0\\\\aug_5.jpg', 0), ('Dataset_Augmented_max\\\\N0\\\\n0_3.jpeg', 0)]\n",
      "[('Dataset_Augmented_max\\\\N0\\\\n0_37.jpeg', 0), ('Dataset_Augmented_max\\\\N0\\\\n0_39.jpeg', 0), ('Dataset_Augmented_max\\\\N0\\\\n0_49.jpeg', 0), ('Dataset_Augmented_max\\\\N0\\\\n0_65.jpeg', 0), ('Dataset_Augmented_max\\\\N0\\\\aug_35.jpg', 0)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "base_dir = \"Dataset_Augmented_max\"\n",
    "\n",
    "train_ratio = 0.8 \n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "for label in os.listdir(base_dir):\n",
    "    class_dir = os.path.join(base_dir, label)\n",
    "    images = os.listdir(class_dir)\n",
    "    \n",
    "    images_with_labels = [(os.path.join(class_dir, img), int(label[1:])) for img in images]\n",
    "    \n",
    "    train_images, test_images = train_test_split(images_with_labels, train_size=train_ratio, random_state=42)\n",
    "    \n",
    "    train_data.extend(train_images)\n",
    "    test_data.extend(test_images)\n",
    "\n",
    "print(f\"Total entrenamiento: {len(train_data)} imágenes\")\n",
    "print(f\"Total prueba: {len(test_data)} imágenes\")\n",
    "\n",
    "print(train_data[:5]) \n",
    "print(test_data[:5])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class NeuronalNetwork(nn.Module):\n",
    "    def __init__(self, num_classes, lstm_hidden, lstm_layers):\n",
    "        super(NeuronalNetwork, self).__init__()\n",
    "        resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        self.resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        self.lstm = nn.LSTM(2048, lstm_hidden, lstm_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(lstm_hidden, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        featueres = self.resnet(x)\n",
    "        featueres = featueres.view(featueres.size(0), -1)\n",
    "        featueres = featueres.unsqueeze(1)\n",
    "\n",
    "        lstm_out, (hn, cn)= self.lstm(featueres)\n",
    "        out = self.fc(hn[-1])\n",
    "        return out\n",
    "\n",
    "model = NeuronalNetwork(num_classes=31, lstm_hidden=128, lstm_layers=2).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Batch(1/119): loss=3.4366\n",
      "Batch(101/119): loss=2.8121\n",
      "Batch(119/119): loss=2.8797\n",
      "Train Values: \n",
      "\t Accuracy: 47.0%, Avg loss: 3.146202 \n",
      "\n",
      "\t\tValidation Values: \n",
      "\t\t Accuracy: 89.8%, Avg loss: 2.675787 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Batch(1/119): loss=2.6681\n",
      "Batch(101/119): loss=1.9451\n",
      "Batch(119/119): loss=1.8154\n",
      "Train Values: \n",
      "\t Accuracy: 92.3%, Avg loss: 2.262032 \n",
      "\n",
      "\t\tValidation Values: \n",
      "\t\t Accuracy: 97.4%, Avg loss: 1.782062 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Batch(1/119): loss=1.7710\n",
      "Batch(101/119): loss=1.1953\n",
      "Batch(119/119): loss=1.1709\n",
      "Train Values: \n",
      "\t Accuracy: 96.6%, Avg loss: 1.406881 \n",
      "\n",
      "\t\tValidation Values: \n",
      "\t\t Accuracy: 97.8%, Avg loss: 1.026566 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class NumbersDataset(Dataset):\n",
    "    def __init__(self, dataset, transform=None):\n",
    "        self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.images = []\n",
    "        self.labels = []\n",
    "\n",
    "        for (img, label) in self.dataset:\n",
    "            self.images.append(img)\n",
    "            self.labels.append(label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.images[idx]).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, self.labels[idx]\n",
    "    \n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, train_loss, val_loss):\n",
    "        if (val_loss - train_loss) > self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    train_size = len(dataloader.dataset)\n",
    "    batch = len(dataloader)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    loss_train, accuracy = 0, 0\n",
    "    \n",
    "    for b, (img, label) in enumerate(dataloader):\n",
    "        img, label = img.to(device), label.to(device)\n",
    "        \n",
    "        pred = model(img)\n",
    "        loss = loss_fn(pred, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_train += loss.item()\n",
    "        accuracy += (pred.argmax(1) == label).type(torch.float).sum().item()\n",
    "        \n",
    "        if b % 100 == 0 or b == len(dataloader) - 1:\n",
    "            print(f\"Batch({b + 1}/{batch}): loss={loss.item():.4f}\")\n",
    "\n",
    "    loss_train /= batch\n",
    "    accuracy /= train_size\n",
    "\n",
    "    print(f\"Train Values: \\n\\t Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {loss_train:>8f} \\n\")\n",
    "    return loss_train\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    val_size = len(dataloader.dataset)\n",
    "    batch = len(dataloader)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    loss_val, accuracy = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, label in dataloader:\n",
    "            img, label = img.to(device), label.to(device)\n",
    "            \n",
    "            pred = model(img)\n",
    "            loss_val += loss_fn(pred, label).item()\n",
    "            accuracy += (pred.argmax(1) == label).type(torch.float).sum().item()\n",
    "\n",
    "    loss_val /= batch\n",
    "    accuracy /= val_size\n",
    "\n",
    "    print(f\"\\t\\tValidation Values: \\n\\t\\t Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {loss_val:>8f} \\n\")\n",
    "    return loss_val\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])  \n",
    "\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "\n",
    "dataset_train = NumbersDataset(train_data, transform)\n",
    "dataset_test = NumbersDataset(test_data, transform)\n",
    "\n",
    "train_dataset = DataLoader(dataset_train, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "early_stopping = EarlyStopping(1, 0.001)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "    early_stopping(\n",
    "        train(train_dataset, model, loss_function, optimizer),\n",
    "        test(val_dataset, model, loss_function)\n",
    "    )\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "img = Image.open(\"prueba/26.jpeg\").convert('RGB')\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "img = transform_val(img).unsqueeze(0).to(device)\n",
    "model.eval()\n",
    "value = model(img).argmax(1).item()\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_max.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(model, \"model_max.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
